<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="canonical" href="https://tmo324.github.io/research.html" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" type="image/x-icon" href="misc/favicon.ico" />
<title>Research - Tergel Molom-Ochir</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Tergel Molom-Ochir</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="activities.html">Activities</a></div>
<div class="menu-item"><a href="awards.html">Awards</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research</h1>
</div>

<h2>Associative Memory Architectures for Intelligent Systems</h2>

<p>My research investigates <b>associative memory</b>&mdash;the ability to retrieve stored patterns from partial or noisy queries&mdash;as a unifying computational primitive across digital, analog, and biological substrates. This work bridges computer architecture, machine learning, and neuromorphic computing.</p>

<h2>Research Areas</h2>

<h3>1. Digital Associative Memory (CAM-based)</h3>
<p>Content-Addressable Memories (CAMs) enable massively parallel search&mdash;the same primitive underlying attention in Transformers. My work explores how CAM architectures can accelerate AI workloads:</p>
<ul>
<li><p><b>CAMformer:</b> Reframing attention as associative memory lookup, enabling hardware-efficient Transformer architectures.</p></li>
<li><p><b>NP-CAM:</b> Scalable CAM architectures for genomics applications (DNA classification). <i>[HPCA 2025]</i></p></li>
<li><p><b>MonoSparse-CAM:</b> Exploiting monotonicity and sparsity for efficient tree model inference. <i>[ISCAS 2025]</i></p></li>
<li><p><b>CAM Survey:</b> Comprehensive review of CAM circuits and their applications in AI. <i>[IEEE TCAS-I]</i></p></li>
</ul>

<h3>2. Analog Associative Memory (Memristive/In-Memory Computing)</h3>
<p>Memristive crossbar arrays naturally implement matrix operations and energy-based dynamics. My work at HPE Labs focuses on:</p>
<ul>
<li><p><b>In-Memory Computing for MCTS:</b> Novel IMC architectures for Monte Carlo Tree Search&mdash;the planning algorithm behind AlphaGo. <i>[Patent Pending]</i></p></li>
<li><p><b>DirectGeMM:</b> Eliminating bottlenecks in analog in-memory computing.</p></li>
</ul>

<h3>3. Biological Associative Memory (Spiking/Neuromorphic)</h3>
<p>Biological neural circuits achieve remarkable efficiency through event-driven, spike-based computation. My collaborations explore:</p>
<ul>
<li><p><b>Neuromorphic Circuits:</b> Hardware implementations of biologically-plausible neuron models.</p></li>
<li><p><b>Neuro-Symbolic Computing:</b> Combining neural networks with symbolic reasoning using in-memory architectures.</p></li>
</ul>

<h2>Collaborators</h2>
<ul>
<li><p><a href="https://ece.duke.edu/faculty/yiran-chen" target="_blank">Dr. Yiran Chen</a> &mdash; Duke University (Advisor)</p></li>
<li><p><a href="https://ece.duke.edu/faculty/hai-helen-li" target="_blank">Dr. Hai "Helen" Li</a> &mdash; Duke University (Advisor)</p></li>
<li><p><a href="https://nano.ecs.umass.edu/" target="_blank">Dr. Qiangfei Xia</a> &mdash; UMass Amherst</p></li>
<li><p>Dr. Aishwarya Natarajan &mdash; Hewlett Packard Labs</p></li>
</ul>

</td>
</tr>
</table>
</body>
</html>
